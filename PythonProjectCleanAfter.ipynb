{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66cbfbd8-21fc-4db3-a265-fb59b33a4e4f",
   "metadata": {},
   "source": [
    "Importing all packages used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6027a1bd-5a39-4fc6-88ce-0b12495f6057",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\remya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "#import emoji\n",
    "import nltk\n",
    "nltk.download('words')\n",
    "words = set(nltk.corpus.words.words())\n",
    "from nltk.tokenize import word_tokenize\n",
    "#import contractions\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy.sparse import hstack\n",
    "import random\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9a897e-8912-46a2-b9e7-c0ebfe535d8d",
   "metadata": {},
   "source": [
    "Reading the tsv into lists for text and class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16d76d11-6740-4553-bd3a-6bb41b3742aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "iFile = open('train.tsv', encoding=\"utf8\")\n",
    "iCSV = csv.reader(iFile, delimiter='\\t')\n",
    "\n",
    "X_text = []\n",
    "y = []\n",
    "\n",
    "for row in iCSV:\n",
    "    X_text.append(row[1])\n",
    "    y.append(row[2])\n",
    "    \n",
    "iFile.close()\n",
    "\n",
    "iFile = open('test.tsv', encoding=\"utf8\")\n",
    "iCSV = csv.reader(iFile, delimiter='\\t')\n",
    "\n",
    "X_text_test = []\n",
    "y_test = []\n",
    "\n",
    "for row in iCSV:\n",
    "    X_text_test.append(row[1])\n",
    "    y_test.append(row[2])\n",
    "    \n",
    "iFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abab1072-ba1f-467f-939e-d7c0c70393f9",
   "metadata": {},
   "source": [
    "Viewing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bcf7897-5711-4516-a1e4-a0ec26e1d17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@USER She should ask a few native Americans what their take on this is. UNT\n",
      "@USER @USER Go home youâ€™re drunk!!! @USER #MAGA #Trump2020 ðŸ‘ŠðŸ‡ºðŸ‡¸ðŸ‘Š URL TIN\n",
      "Amazon is investigating Chinese employees who are selling internal data to third-party sellers looking for an edge in the competitive marketplace. URL #Amazon #MAGA #KAG #CHINA #TCOT NOT\n",
      "@USER Someone should'veTaken\" this piece of shit to a volcano. ðŸ˜‚\" UNT\n",
      "@USER @USER Obama wanted liberals &amp; illegals to move into red states NOT\n",
      "@USER Liberals are all Kookoo !!! TIN\n",
      "@USER @USER Oh noes! Tough shit. UNT\n",
      "@USER was literally just talking about this lol all mass shootings like that have been set ups. itâ€™s propaganda used to divide us on major issues like gun control and terrorism TIN\n",
      "@USER Buy more icecream!!! NOT\n",
      "@USER Canada doesnâ€™t need another CUCK! We already have enough #LooneyLeft #Liberals f**king up our great country! #Qproofs #TrudeauMustGo TIN\n",
      "@USER @USER @USER Itâ€™s not my fault you support gun control NOT\n",
      "@USER Whatâ€™s the difference between #Kavanaugh and @USER   One of these men admitted to groping a 15 year old girl years ago.  The other is going to be #confirmed to the SCJ   #DemsareFrauds #DemsAreDone   #WalkAwayDemocrats2018 #redwave #VoteRedSaveAmerica #trumptrain #MAGA URL NOT\n",
      "@USER you are a lying corrupt traitor!!! Nobody wants to hear anymore of your lies!!! #DeepStateCorruption URL TIN\n",
      "@USER @USER @USER It should scare every American!  She is playing Hockey with a warped puck! NOT\n",
      "@USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER I like my soda like I like my boarders with a lot of ICE. NOT\n",
      "@USER you are also the king of taste NOT\n",
      "#MAGA @USER  ðŸŽ¶ Sing like no one is listening  â™¥ï¸ Love like you've never been hurt  âœ”ï¸ Vote GOP when no one is watching  ðŸ™‰  And don't listen to Liberals' dirt URL NOT\n",
      "5/5: @USER The time is right for this House to respond to the concerns of all Canadians. Four out of five Canadians support stronger gun control and with good reason.â€ #guncontrol #cdnpoli #cdnhist\" NOT\n",
      "@USER Besides Jaxâ€™s mom and maybe Ope he is hands down my favorite heâ€™s like the only good person on the show ðŸ˜‚ NOT\n",
      "@USER @USER @USER gun control! That is all these kids are asking for! TIN\n",
      "\n",
      "\n",
      "\n",
      "@USER @USER Go home youâ€™re drunk!!! @USER #MAGA #Trump2020 ðŸ‘ŠðŸ‡ºðŸ‡¸ðŸ‘Š URL TIN\n",
      "@USER Liberals are all Kookoo !!! TIN\n",
      "@USER was literally just talking about this lol all mass shootings like that have been set ups. itâ€™s propaganda used to divide us on major issues like gun control and terrorism TIN\n",
      "@USER Canada doesnâ€™t need another CUCK! We already have enough #LooneyLeft #Liberals f**king up our great country! #Qproofs #TrudeauMustGo TIN\n",
      "@USER you are a lying corrupt traitor!!! Nobody wants to hear anymore of your lies!!! #DeepStateCorruption URL TIN\n",
      "@USER @USER @USER gun control! That is all these kids are asking for! TIN\n",
      "@USER @USER @USER @USER LOL!!!   Throwing the BULLSHIT Flag on such nonsense!!  #PutUpOrShutUp   #Kavanaugh   #MAGA   #CallTheVoteAlready URL TIN\n",
      "@USER @USER Kind of like when conservatives wanna associate everyone to their left as communist antifa members? TIN\n",
      "@USER @USER Da fuck is going on people?   There's the men's room and the women's room  Pick one and stick w it ðŸ¤” TIN\n",
      "@USER Tbh these days i just don't like people in general i just don't connect with people these days just a annoyance.. TIN\n",
      "The only thing the Democrats have is lying and stalling to stop Trump from being #President.  What have they done for you lately. #Trump #Kavanaugh #MAGA #DEMSUCK TIN\n",
      "@USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER You are not very smart are you? Why do you think Gen Flynnâ€™s sentencing keeps being rescheduled? URL TIN\n",
      "@USER @USER @USER That's expected if you placate the violent leftists/terrorists. #KavanaughConfirmation #woke TIN\n",
      "4 out of 10 British people are basically full-on racists. 4 out of 10 voters vote for the Conservatives. Coincidence!???!??? TIN\n",
      "@USER Fuck off TIN\n",
      "@USER @USER The prison system is so fucked.  Why are they still getting away with what is potentially murder with intent if the prisoners die in the hurricane? They did this in Louisiana and like 500 inmates went missing\"\" TIN\n",
      "@USER @USER She is useless.  Attempts to do the right thing but never follows through. TIN\n",
      "@USER Grateful Trump doesnâ€™t have a dog in the White House. He is a cruel man. TIN\n",
      "@USER Trump kicks dem butt - its so fun. TIN\n",
      "@USER @USER @USER He is dumb and dumber all in one president.  It's a two for one. TIN\n",
      "@USER And this from the clown that should be in prison? TIN\n",
      "@USER Thats because you are an old man. TIN\n",
      "@USER @USER @USER They are obstructionists. Anti-american. Pharisees. Ask them what they are running on and you get nothing. Maga! TIN\n",
      "@USER I mean it worked for gun control right? URL TIN\n"
     ]
    }
   ],
   "source": [
    "for tweet,Class in zip(X_text[:20],y[:20]):\n",
    "    print(tweet,Class)\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "for tweet,Class in zip(X_text[:75],y[:75]):\n",
    "    if Class == 'TIN':\n",
    "        print(tweet,Class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff9543c-3e83-45ac-abfc-1fce59c19939",
   "metadata": {},
   "source": [
    "Determining class balances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c7bd007-975f-48ce-b2a8-a13e252e4c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNT freq: 0.039746978851963745, TIN freq: 0.29286253776435045, NOT freq: 0.6673904833836858\n"
     ]
    }
   ],
   "source": [
    "UNT = 0 \n",
    "TIN = 0\n",
    "NOT = 0 \n",
    "TOT = 0\n",
    "\n",
    "for val in y:\n",
    "    TOT += 1\n",
    "    if val == 'UNT':\n",
    "        UNT += 1\n",
    "    elif val == 'TIN':\n",
    "        TIN += 1\n",
    "    else:\n",
    "        NOT +=1 \n",
    "        \n",
    "        \n",
    "print(\"UNT freq: {}, TIN freq: {}, NOT freq: {}\".format(UNT/TOT,TIN/TOT,NOT/TOT))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eab8e8b-5b63-48ff-b5d3-13f2820510be",
   "metadata": {},
   "source": [
    "Creating a train and validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b56c061-9840-44fc-954b-0d565b68b946",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_txt, X_val_txt, y_train_txt, y_val_txt = train_test_split(X_text, y, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3bd09f-5660-47a3-85ad-73e6b0c89819",
   "metadata": {},
   "source": [
    "Creating a boolean feature that records 1 if the tweet has an ! and a 0 if it does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eae47f1b-d293-4c7c-a39e-2ff1038e110a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\remya\\AppData\\Local\\Temp/ipykernel_13452/4040373523.py:23: DeprecationWarning: scipy.array is deprecated and will be removed in SciPy 2.0.0, use numpy.array instead\n",
      "  train_ex = sp.array(ex_feature_train)\n",
      "C:\\Users\\remya\\AppData\\Local\\Temp/ipykernel_13452/4040373523.py:24: DeprecationWarning: scipy.array is deprecated and will be removed in SciPy 2.0.0, use numpy.array instead\n",
      "  val_ex = sp.array(ex_feature_val)\n",
      "C:\\Users\\remya\\AppData\\Local\\Temp/ipykernel_13452/4040373523.py:25: DeprecationWarning: scipy.array is deprecated and will be removed in SciPy 2.0.0, use numpy.array instead\n",
      "  test_ex = sp.array(ex_feature_test)\n"
     ]
    }
   ],
   "source": [
    "ex_feature_train = []\n",
    "ex_feature_val = []\n",
    "ex_feature_test = []\n",
    "\n",
    "for tweet in X_train_txt:\n",
    "    if '!' in tweet:\n",
    "        ex_feature_train.append([1])\n",
    "    else:\n",
    "        ex_feature_train.append([0])\n",
    "\n",
    "for tweet in X_val_txt:\n",
    "    if '!' in tweet:\n",
    "        ex_feature_val.append([1])\n",
    "    else:\n",
    "        ex_feature_val.append([0])\n",
    "        \n",
    "for tweet in X_text_test:\n",
    "    if '!' in tweet:\n",
    "        ex_feature_test.append([1])\n",
    "    else:\n",
    "        ex_feature_test.append([0])\n",
    "        \n",
    "train_ex = sp.array(ex_feature_train)\n",
    "val_ex = sp.array(ex_feature_val)\n",
    "test_ex = sp.array(ex_feature_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0002e38-6b52-4617-b411-3689979b2c69",
   "metadata": {},
   "source": [
    "Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99e66ae-743c-4dae-9c4b-ed14dd6eae60",
   "metadata": {},
   "source": [
    "Creating a dictionary to replace common contractions with the full words they represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e467cf02-49ff-49b3-9d3a-a5202562ef85",
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_dict = {\n",
    "\n",
    "\"ainâ€™t\": \"is not\",\n",
    " \n",
    "\"arenâ€™t\": \"are not\",\n",
    "\n",
    "\"canâ€™t\": \"cannot\",\n",
    "\n",
    "\"canâ€™tâ€™ve\": \"cannot have\",\n",
    "\n",
    "\"â€™cause\": \"because\",\n",
    "\n",
    "\"couldâ€™ve\": \"could have\",\n",
    "\n",
    "\"couldnâ€™t\": \"could not\",\n",
    "\n",
    "\"couldnâ€™tâ€™ve\": \"could not have\",\n",
    "\n",
    "\"didnâ€™t\": \"did not\",\n",
    "\n",
    "\"doesnâ€™t\": \"does not\",\n",
    "\n",
    "\"donâ€™t\": \"do not\",\n",
    "\n",
    "\"hadnâ€™t\": \"had not\",\n",
    "\n",
    "\"hadnâ€™tâ€™ve\": \"had not have\",\n",
    "\n",
    "\"hasnâ€™t\": \"has not\",\n",
    "\n",
    "\"havenâ€™t\": \"have not\",\n",
    "\n",
    "\"heâ€™d\": \"he would\",\n",
    "\n",
    "\"heâ€™dâ€™ve\": \"he would have\",\n",
    "\n",
    "\"heâ€™ll\": \"he will\",\n",
    "\n",
    "\"heâ€™llâ€™ve\": \"he he will have\",\n",
    "\n",
    "\"heâ€™s\": \"he is\",\n",
    "\n",
    "\"howâ€™d\": \"how did\",\n",
    "\n",
    "\"howâ€™dâ€™y\": \"how do you\",\n",
    "\n",
    "\"howâ€™ll\": \"how will\",\n",
    "\n",
    "\"howâ€™s\": \"how is\",\n",
    "\n",
    "\"Iâ€™d\": \"I would\",\n",
    "\n",
    "\"Iâ€™dâ€™ve\": \"I would have\",\n",
    "\n",
    "\"Iâ€™ll\": \"I will\",\n",
    "\n",
    "\"Iâ€™llâ€™ve\": \"I will have\",\n",
    "\n",
    "\"Iâ€™m\": \"I am\",\n",
    "\n",
    "\"Iâ€™ve\": \"I have\",\n",
    "\n",
    "\"iâ€™d\": \"i would\",\n",
    "\n",
    "\"iâ€™dâ€™ve\": \"i would have\",\n",
    "\n",
    "\"iâ€™ll\": \"i will\",\n",
    "\n",
    "\"iâ€™llâ€™ve\": \"i will have\",\n",
    "\n",
    "\"iâ€™m\": \"i am\",\n",
    "\n",
    "\"iâ€™ve\": \"i have\",\n",
    "\n",
    "\"isnâ€™t\": \"is not\",\n",
    "\n",
    "\"itâ€™d\": \"it would\",\n",
    "\n",
    "\"itâ€™dâ€™ve\": \"it would have\",\n",
    "\n",
    "\"itâ€™ll\": \"it will\",\n",
    "\n",
    "\"itâ€™llâ€™ve\": \"it will have\",\n",
    "\n",
    "\"itâ€™s\": \"it is\",\n",
    "\n",
    "\"letâ€™s\": \"let us\",\n",
    "\n",
    "\"maâ€™am\": \"madam\",\n",
    "\n",
    "\"maynâ€™t\": \"may not\",\n",
    "\n",
    "\"mightâ€™ve\": \"might have\",\n",
    "\n",
    "\"mightnâ€™t\": \"might not\",\n",
    "\n",
    "\"mightnâ€™tâ€™ve\": \"might not have\",\n",
    "\n",
    "\"mustâ€™ve\": \"must have\",\n",
    "\n",
    "\"mustnâ€™t\": \"must not\",\n",
    "\n",
    "\"mustnâ€™tâ€™ve\": \"must not have\",\n",
    "\n",
    "\"neednâ€™t\": \"need not\",\n",
    "\n",
    "\"neednâ€™tâ€™ve\": \"need not have\",\n",
    "\n",
    "\"oâ€™clock\": \"of the clock\",\n",
    "\n",
    "\"oughtnâ€™t\": \"ought not\",\n",
    "\n",
    "\"oughtnâ€™tâ€™ve\": \"ought not have\",\n",
    "\n",
    "\"shanâ€™t\": \"shall not\",\n",
    "\n",
    "\"shaâ€™nâ€™t\": \"shall not\",\n",
    "\n",
    "\"shanâ€™tâ€™ve\": \"shall not have\",\n",
    "\n",
    "\"sheâ€™d\": \"she would\",\n",
    "\n",
    "\"sheâ€™dâ€™ve\": \"she would have\",\n",
    "\n",
    "\"sheâ€™ll\": \"she will\",\n",
    "\n",
    "\"sheâ€™llâ€™ve\": \"she will have\",\n",
    "\n",
    "\"sheâ€™s\": \"she is\",\n",
    "\n",
    "\"shouldâ€™ve\": \"should have\",\n",
    "\n",
    "\"shouldnâ€™t\": \"should not\",\n",
    "\n",
    "\"shouldnâ€™tâ€™ve\": \"should not have\",\n",
    "\n",
    "\"soâ€™ve\": \"so have\",\n",
    "\n",
    "\"soâ€™s\": \"so as\",\n",
    "\n",
    "\"thatâ€™d\": \"that would\",\n",
    "\n",
    "\"thatâ€™dâ€™ve\": \"that would have\",\n",
    "\n",
    "\"thatâ€™s\": \"that is\",\n",
    "\n",
    "\"thereâ€™d\": \"there would\",\n",
    "\n",
    "\"thereâ€™dâ€™ve\": \"there would have\",\n",
    "\n",
    "\"thereâ€™s\": \"there is\",\n",
    "\n",
    "\"theyâ€™d\": \"they would\",\n",
    "\n",
    "\"theyâ€™dâ€™ve\": \"they would have\",\n",
    "\n",
    "\"theyâ€™ll\": \"they will\",\n",
    "\n",
    "\"theyâ€™llâ€™ve\": \"they will have\",\n",
    "\n",
    "\"theyâ€™re\": \"they are\",\n",
    "\n",
    "\"theyâ€™ve\": \"they have\",\n",
    "\n",
    "\"toâ€™ve\": \"to have\",\n",
    "\n",
    "\"wasnâ€™t\": \"was not\",\n",
    "\n",
    "\"weâ€™d\": \"we would\",\n",
    "\n",
    "\"weâ€™dâ€™ve\": \"we would have\",\n",
    "\n",
    "\"weâ€™ll\": \"we will\",\n",
    "\n",
    "\"weâ€™llâ€™ve\": \"we will have\",\n",
    "\n",
    "\"weâ€™re\": \"we are\",\n",
    "\n",
    "\"weâ€™ve\": \"we have\",\n",
    "\n",
    "\"werenâ€™t\": \"were not\",\n",
    "\n",
    "\"whatâ€™ll\": \"what will\",\n",
    "\n",
    "\"whatâ€™llâ€™ve\": \"what will have\",\n",
    "\n",
    "\"whatâ€™re\": \"what are\",\n",
    "\n",
    "\"whatâ€™s\": \"what is\",\n",
    "\n",
    "\"whatâ€™ve\": \"what have\",\n",
    "\n",
    "\"whenâ€™s\": \"when is\",\n",
    "\n",
    "\"whenâ€™ve\": \"when have\",\n",
    "\n",
    "\"whereâ€™d\": \"where did\",\n",
    "\n",
    "\"whereâ€™s\": \"where is\",\n",
    "\n",
    "\"whereâ€™ve\": \"where have\",\n",
    "\n",
    "\"whoâ€™ll\": \"who will\",\n",
    "\n",
    "\"whoâ€™llâ€™ve\": \"who will have\",\n",
    "\n",
    "\"whoâ€™s\": \"who is\",\n",
    "\n",
    "\"whoâ€™ve\": \"who have\",\n",
    "\n",
    "\"whyâ€™s\": \"why is\",\n",
    "\n",
    "\"whyâ€™ve\": \"why have\",\n",
    "\n",
    "\"willâ€™ve\": \"will have\",\n",
    "\n",
    "\"wonâ€™t\": \"will not\",\n",
    "\n",
    "\"wonâ€™tâ€™ve\": \"will not have\",\n",
    "\n",
    "\"wouldâ€™ve\": \"would have\",\n",
    "\n",
    "\"wouldnâ€™t\": \"would not\",\n",
    "\n",
    "\"wouldnâ€™tâ€™ve\": \"would not have\",\n",
    "\n",
    "\"yâ€™all\": \"you all\",\n",
    "\n",
    "\"yâ€™allâ€™d\": \"you all would\",\n",
    "\n",
    "\"yâ€™allâ€™dâ€™ve\": \"you all would have\",\n",
    "\n",
    "\"yâ€™allâ€™re\": \"you all are\",\n",
    "\n",
    "\"yâ€™allâ€™ve\": \"you all have\",\n",
    "\n",
    "\"youâ€™d\": \"you would\",\n",
    "\n",
    "\"youâ€™dâ€™ve\": \"you would have\",\n",
    "\n",
    "\"youâ€™ll\": \"you will\",\n",
    "\n",
    "\"youâ€™llâ€™ve\": \"you will have\",\n",
    "\n",
    "\"youâ€™re\": \"you are\",\n",
    "\n",
    "\"youâ€™ve\": \"you have\"}\n",
    "\n",
    "def replace_contraction(tweet):\n",
    "    replaceList = []\n",
    "    tweetList = tweet.split()\n",
    "    for word in tweetList:\n",
    "        if word in contraction_dict.keys():\n",
    "            replaceList.append(contraction_dict[word])\n",
    "        else:\n",
    "            replaceList.append(word)\n",
    "    return \" \".join(replaceList)\n",
    "X_train_pre_clean = []  \n",
    "X_val_pre_clean = []\n",
    "X_test_pre_clean = []\n",
    "for tweet in X_train_txt:\n",
    "    X_train_pre_clean.append(replace_contraction(tweet))\n",
    "for tweet in X_val_txt:\n",
    "    X_val_pre_clean.append(replace_contraction(tweet)) \n",
    "for tweet in X_text_test:\n",
    "    X_test_pre_clean.append(replace_contraction(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63c07a57-255a-40b7-82c2-f10b0e130517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(\"@[A-Za-z0-9]+\",\"\",tweet) #Remove @ sign\n",
    "    tweet = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", tweet) #Remove http links\n",
    "    tweet = \" \".join(tweet.split())\n",
    "    tweet = tweet.replace(\"#\", \"\").replace(\"_\", \" \") #Remove hashtag sign but keep the text\n",
    "    tweet = re.sub('[.()!?]', '', tweet)\n",
    "    tweet = re.sub('\\[.*?\\]','', tweet)\n",
    "\n",
    "    return tweet\n",
    "X_train_clean = []\n",
    "X_val_clean = []\n",
    "X_test_clean = []\n",
    "\n",
    "for tweet in X_train_pre_clean:\n",
    "    X_train_clean.append(cleaner(tweet))\n",
    "    \n",
    "for tweet in X_val_pre_clean:\n",
    "    X_val_clean.append(cleaner(tweet))\n",
    "\n",
    "for tweet in X_test_pre_clean:\n",
    "    X_test_clean.append(cleaner(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fa2f54-3563-4d80-92e7-531d0c7a05b0",
   "metadata": {},
   "source": [
    "Reading in an offensive word lexicon and creating a feature that counts the number of offensive words in each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4aac92bf-56fe-4c47-8210-c9f64605e702",
   "metadata": {},
   "outputs": [],
   "source": [
    "offensive_words = []\n",
    "\n",
    "iFile = open('Offensive words.txt')\n",
    "\n",
    "for word in iFile:\n",
    "    offensive_words.append(word.strip().lower())\n",
    "\n",
    "iFile.close()\n",
    "offensive_words = offensive_words[0:212]\n",
    "offensive_words = set(offensive_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29af174d-c5cf-41bb-93e3-07e426bcfb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\remya\\AppData\\Local\\Temp/ipykernel_13452/1377861510.py:9: DeprecationWarning: scipy.array is deprecated and will be removed in SciPy 2.0.0, use numpy.array instead\n",
      "  offensive_train_word_array = sp.array(offensive_train_word_feature)\n",
      "C:\\Users\\remya\\AppData\\Local\\Temp/ipykernel_13452/1377861510.py:19: DeprecationWarning: scipy.array is deprecated and will be removed in SciPy 2.0.0, use numpy.array instead\n",
      "  offensive_val_word_array = sp.array(offensive_val_word_feature)\n",
      "C:\\Users\\remya\\AppData\\Local\\Temp/ipykernel_13452/1377861510.py:29: DeprecationWarning: scipy.array is deprecated and will be removed in SciPy 2.0.0, use numpy.array instead\n",
      "  offensive_test_word_array = sp.array(offensive_test_word_feature)\n"
     ]
    }
   ],
   "source": [
    "offensive_train_word_feature = []\n",
    "\n",
    "for tweet in X_train_clean:\n",
    "    temp = 0\n",
    "    for word in tweet.split():\n",
    "        if word.lower() in offensive_words:\n",
    "            temp += 1\n",
    "    offensive_train_word_feature.append([temp]) \n",
    "offensive_train_word_array = sp.array(offensive_train_word_feature)\n",
    "\n",
    "offensive_val_word_feature = [] \n",
    "\n",
    "for tweet in X_val_clean:\n",
    "    temp = 0\n",
    "    for word in tweet.split():\n",
    "        if word.lower() in offensive_words:\n",
    "            temp += 1\n",
    "    offensive_val_word_feature.append([temp]) \n",
    "offensive_val_word_array = sp.array(offensive_val_word_feature)\n",
    "\n",
    "offensive_test_word_feature = []\n",
    "\n",
    "for tweet in X_test_clean:\n",
    "    temp = 0\n",
    "    for word in tweet.split():\n",
    "        if word.lower() in offensive_words:\n",
    "            temp += 1\n",
    "    offensive_test_word_feature.append([temp])\n",
    "offensive_test_word_array = sp.array(offensive_test_word_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f38ee1-82f7-41bc-bad6-b9f35dff95ec",
   "metadata": {},
   "source": [
    "Reading in a positive word lexicon and creating a feature that counts the number of positive words in each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cda480ee-4f23-45a7-a94e-3ccd0135595c",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_words = []\n",
    "\n",
    "iFile = open('positive-words.txt')\n",
    "\n",
    "for word in iFile:\n",
    "    positive_words.append(word.strip().lower())\n",
    "\n",
    "iFile.close()\n",
    "positive_words = set(positive_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d93bdc0-badf-498c-870d-4a2e3e13bf3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\remya\\AppData\\Local\\Temp/ipykernel_13452/344155394.py:10: DeprecationWarning: scipy.array is deprecated and will be removed in SciPy 2.0.0, use numpy.array instead\n",
      "  positive_train_word_feature = sp.array(positive_train_word_list)\n",
      "C:\\Users\\remya\\AppData\\Local\\Temp/ipykernel_13452/344155394.py:20: DeprecationWarning: scipy.array is deprecated and will be removed in SciPy 2.0.0, use numpy.array instead\n",
      "  positive_val_word_feature = sp.array(positive_val_word_list)\n",
      "C:\\Users\\remya\\AppData\\Local\\Temp/ipykernel_13452/344155394.py:30: DeprecationWarning: scipy.array is deprecated and will be removed in SciPy 2.0.0, use numpy.array instead\n",
      "  positive_word_test_feature = sp.array(positive_word_test_list)\n"
     ]
    }
   ],
   "source": [
    "positive_train_word_list = []\n",
    "\n",
    "for tweet in X_train_clean:\n",
    "    temp = 0\n",
    "    for word in tweet.split():\n",
    "        if word.lower() in positive_words:\n",
    "            temp += 1\n",
    "    positive_train_word_list.append([temp]) \n",
    "    \n",
    "positive_train_word_feature = sp.array(positive_train_word_list)\n",
    "\n",
    "positive_val_word_list = [] \n",
    "\n",
    "for tweet in X_val_clean:\n",
    "    temp = 0\n",
    "    for word in tweet.split():\n",
    "        if word.lower() in positive_words:\n",
    "            temp += 1\n",
    "    positive_val_word_list.append([temp]) \n",
    "positive_val_word_feature = sp.array(positive_val_word_list)\n",
    "\n",
    "positive_word_test_list = []\n",
    "\n",
    "for tweet in X_test_clean:\n",
    "    temp = 0\n",
    "    for word in tweet.split():\n",
    "        if word.lower() in positive_words:\n",
    "            temp += 1 \n",
    "    positive_word_test_list.append([temp])\n",
    "positive_word_test_feature = sp.array(positive_word_test_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c7a32f-997a-4d8d-8c14-77852a9348ad",
   "metadata": {},
   "source": [
    "Reading in a lexicon and creating a feature that counts words we found were highly associated with TIN in each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58fce382-41f0-41d5-85f3-ad2190a80c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "tin_wordsList = []\n",
    "iFile = open('TINGroups.txt')\n",
    "\n",
    "for word in iFile:\n",
    "    tin_wordsList.append(word.strip().lower())\n",
    "\n",
    "iFile.close()\n",
    "tin_words = set(tin_wordsList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d68b628d-ce3c-498e-8e42-5539d4b8437f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\remya\\AppData\\Local\\Temp/ipykernel_13452/2245093005.py:9: DeprecationWarning: scipy.array is deprecated and will be removed in SciPy 2.0.0, use numpy.array instead\n",
      "  tin_word_train_feature = sp.array(train_tin_words)\n",
      "C:\\Users\\remya\\AppData\\Local\\Temp/ipykernel_13452/2245093005.py:19: DeprecationWarning: scipy.array is deprecated and will be removed in SciPy 2.0.0, use numpy.array instead\n",
      "  tin_word_val_feature = sp.array(val_tin_words)\n",
      "C:\\Users\\remya\\AppData\\Local\\Temp/ipykernel_13452/2245093005.py:29: DeprecationWarning: scipy.array is deprecated and will be removed in SciPy 2.0.0, use numpy.array instead\n",
      "  tin_words_test_feature = sp.array(tin_words_test_list)\n"
     ]
    }
   ],
   "source": [
    "train_tin_words = []\n",
    "\n",
    "for tweet in X_train_clean:\n",
    "    temp = 0\n",
    "    for word in tweet.split():\n",
    "        if word.lower() in tin_words:\n",
    "            temp += 1\n",
    "    train_tin_words.append([temp]) \n",
    "tin_word_train_feature = sp.array(train_tin_words)\n",
    "\n",
    "val_tin_words = [] \n",
    "\n",
    "for tweet in X_val_clean:\n",
    "    temp = 0\n",
    "    for word in tweet.split():\n",
    "        if word.lower() in tin_words:\n",
    "            temp += 1\n",
    "    val_tin_words.append([temp]) \n",
    "tin_word_val_feature = sp.array(val_tin_words)\n",
    "\n",
    "tin_words_test_list = [] \n",
    "\n",
    "for tweet in X_test_clean:\n",
    "    temp = 0\n",
    "    for word in tweet.split():\n",
    "        if word.lower() in tin_words:\n",
    "            temp += 1\n",
    "    tin_words_test_list.append([temp]) \n",
    "tin_words_test_feature = sp.array(tin_words_test_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840d46ac-c175-41c7-83dc-81ee635ef317",
   "metadata": {},
   "source": [
    "We noticed that a proportion of the tweets containing \"Trump\" are targeted insults. Creating a vector containing the number of occurrences of the word 'trump' in each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a95103e6-70aa-428a-935d-f8772056c67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\remya\\AppData\\Local\\Temp/ipykernel_13452/3106246390.py:26: DeprecationWarning: scipy.array is deprecated and will be removed in SciPy 2.0.0, use numpy.array instead\n",
      "  Trump_count_feature_train = sp.array(Train_trump_count)\n",
      "C:\\Users\\remya\\AppData\\Local\\Temp/ipykernel_13452/3106246390.py:27: DeprecationWarning: scipy.array is deprecated and will be removed in SciPy 2.0.0, use numpy.array instead\n",
      "  Trump_count_feature_val = sp.array(Val_trump_count)\n",
      "C:\\Users\\remya\\AppData\\Local\\Temp/ipykernel_13452/3106246390.py:28: DeprecationWarning: scipy.array is deprecated and will be removed in SciPy 2.0.0, use numpy.array instead\n",
      "  Trump_count_test_feature = sp.array(trump_count_test_list)\n"
     ]
    }
   ],
   "source": [
    "Train_trump_count = []\n",
    "Val_trump_count = []\n",
    "trump_count_test_list = []\n",
    "\n",
    "for tweet in X_train_clean:\n",
    "    count = 0\n",
    "    for word in tweet.lower().split():\n",
    "        if word == 'trump':\n",
    "            count += 1\n",
    "    Train_trump_count.append([count])\n",
    "    \n",
    "for tweet in X_val_clean:\n",
    "    count = 0\n",
    "    for word in tweet.lower().split():\n",
    "        if word == 'trump':\n",
    "            count += 1\n",
    "    Val_trump_count.append([count])\n",
    "    \n",
    "for tweet in X_test_clean:\n",
    "    count = 0\n",
    "    for word in tweet.lower().split():\n",
    "        if word == 'trump':\n",
    "            count += 1\n",
    "    trump_count_test_list.append([count])\n",
    "\n",
    "Trump_count_feature_train = sp.array(Train_trump_count)\n",
    "Trump_count_feature_val = sp.array(Val_trump_count)\n",
    "Trump_count_test_feature = sp.array(trump_count_test_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46f1d43-86d7-4034-a017-3ef4433d2334",
   "metadata": {},
   "source": [
    "Using the nltk library to count the number of personal pronouns in a tweet. The idea is that tweets with proper nouns are more likely to be targeted insults than tweets without proper nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "264e303e-9df5-4e5e-8900-75509eecdca8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\remya/nltk_data'\n    - 'C:\\\\Users\\\\remya\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\remya\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\remya\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\remya\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13452/421164537.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtokenizedList\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mX_train_clean\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtagListofLists\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mList\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mList\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokenizedList\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mPropNounCountList\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13452/421164537.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtokenizedList\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mX_train_clean\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtagListofLists\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mList\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mList\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokenizedList\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mPropNounCountList\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tag\\__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    163\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m     \"\"\"\n\u001b[1;32m--> 165\u001b[1;33m     \u001b[0mtagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tag\\__init__.py\u001b[0m in \u001b[0;36m_get_tagger\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[0mtagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0map_russian_model_loc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[0mtagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tag\\perceptron.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, load)\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m             AP_MODEL_LOC = \"file:\" + str(\n\u001b[1;32m--> 167\u001b[1;33m                 \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"taggers/averaged_perceptron_tagger/\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mPICKLE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m             )\n\u001b[0;32m    169\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAP_MODEL_LOC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"*\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\remya/nltk_data'\n    - 'C:\\\\Users\\\\remya\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\remya\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\remya\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\remya\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "#nltk.tag.pos_tag(tokens, tagset=None, lang='eng')\n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "#nltk.download('punkt')\n",
    "\n",
    "tokenizedList = [word_tokenize(tweet) for tweet in X_train_clean]\n",
    "tagListofLists = [nltk.pos_tag(List) for List in tokenizedList]\n",
    "\n",
    "PropNounCountList = []\n",
    "\n",
    "for List in tagListofLists:\n",
    "    CountVariable = 0\n",
    "    for Tuple in List:\n",
    "        if Tuple[1] == \"PRP\":\n",
    "            CountVariable += 1\n",
    "    PropNounCountList.append([CountVariable])\n",
    "PRPTrainFeat = sp.array(PropNounCountList)\n",
    "\n",
    "tokenizedListVal = [word_tokenize(tweet) for tweet in X_val_clean]\n",
    "tagListofListsVal = [nltk.pos_tag(List) for List in tokenizedListVal]\n",
    "\n",
    "PropNounCountListVal = []\n",
    "\n",
    "for List in tagListofListsVal:\n",
    "    CountVariable = 0\n",
    "    for Tuple in List:\n",
    "        if Tuple[1] == \"PRP\":\n",
    "            CountVariable += 1\n",
    "    PropNounCountListVal.append([CountVariable])\n",
    "PRPValFeat = sp.array(PropNounCountListVal)\n",
    "\n",
    "tokenizedListVal = [word_tokenize(tweet) for tweet in X_test_clean]\n",
    "tagListofListsVal = [nltk.pos_tag(List) for List in tokenizedListVal]\n",
    "\n",
    "PropNounCountListTest = []\n",
    "\n",
    "for List in tagListofListsVal:\n",
    "    CountVariable = 0\n",
    "    for Tuple in List:\n",
    "        if Tuple[1] == \"PRP\":\n",
    "            CountVariable += 1\n",
    "    PropNounCountListTest.append([CountVariable])\n",
    "PRPTestFeat = sp.array(PropNounCountListVal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d7ab15-806b-4e95-8d24-a82f477a1d43",
   "metadata": {},
   "source": [
    "Converting to numpy arrays and creating bag of words matrix. Then experimenting with our different features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa183ba2-0f77-4038-9f3e-d5b7099556b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\remya\\AppData\\Local\\Temp/ipykernel_13452/1477533737.py:2: DeprecationWarning: scipy.array is deprecated and will be removed in SciPy 2.0.0, use numpy.array instead\n",
      "  X_train_clean = sp.array(X_train_clean)\n",
      "C:\\Users\\remya\\AppData\\Local\\Temp/ipykernel_13452/1477533737.py:3: DeprecationWarning: scipy.array is deprecated and will be removed in SciPy 2.0.0, use numpy.array instead\n",
      "  X_val_clean = sp.array(X_val_clean)\n",
      "C:\\Users\\remya\\AppData\\Local\\Temp/ipykernel_13452/1477533737.py:4: DeprecationWarning: scipy.array is deprecated and will be removed in SciPy 2.0.0, use numpy.array instead\n",
      "  y_train = sp.array(y_train_txt)\n",
      "C:\\Users\\remya\\AppData\\Local\\Temp/ipykernel_13452/1477533737.py:5: DeprecationWarning: scipy.array is deprecated and will be removed in SciPy 2.0.0, use numpy.array instead\n",
      "  y_val = sp.array(y_val_txt)\n",
      "C:\\Users\\remya\\AppData\\Local\\Temp/ipykernel_13452/1477533737.py:6: DeprecationWarning: scipy.array is deprecated and will be removed in SciPy 2.0.0, use numpy.array instead\n",
      "  X_test_clean = sp.array(X_test_clean)\n",
      "C:\\Users\\remya\\AppData\\Local\\Temp/ipykernel_13452/1477533737.py:7: DeprecationWarning: scipy.array is deprecated and will be removed in SciPy 2.0.0, use numpy.array instead\n",
      "  y_test = sp.array(y_test)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'PRPTrainFeat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13452/1477533737.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_clean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtin_word_train_feature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moffensive_train_word_array\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPRPTrainFeat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mX_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtin_word_val_feature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moffensive_val_word_array\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPRPValFeat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PRPTrainFeat' is not defined"
     ]
    }
   ],
   "source": [
    "r = TfidfVectorizer(ngram_range=(1,2),max_df = 0.5)\n",
    "X_train_clean = sp.array(X_train_clean)\n",
    "X_val_clean = sp.array(X_val_clean)\n",
    "y_train = sp.array(y_train_txt)\n",
    "y_val = sp.array(y_val_txt)\n",
    "X_test_clean = sp.array(X_test_clean)\n",
    "y_test = sp.array(y_test)\n",
    "\n",
    "X_train = r.fit_transform(X_train_clean)\n",
    "X_val = r.transform(X_val_clean)\n",
    "X_test = r.transform(X_test_clean)\n",
    "\n",
    "X_train = hstack([X_train, tin_word_train_feature, offensive_train_word_array, PRPTrainFeat])\n",
    "\n",
    "X_val = hstack([X_val, tin_word_val_feature, offensive_val_word_array, PRPValFeat])\n",
    "\n",
    "X_test = hstack([X_test, tin_words_test_feature, offensive_test_word_array, PRPTestFeat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2a3bd9dc-93f7-4c34-a466-4997d290817f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = hstack([X_train,\n",
    "                  offensive_train_word_array,\n",
    "                  train_ex,\n",
    "                  Trump_count_feature_train,\n",
    "                  PRPTrainFeat,\n",
    "                  positive_train_word_feature,\n",
    "                  tin_word_train_feature])\n",
    "\n",
    "#X_val = hstack([X_val, \n",
    "                offensive_val_word_array,\n",
    "                val_ex,\n",
    "                Trump_count_feature_val,\n",
    "                PRPValFeat,\n",
    "                positive_val_word_feature,\n",
    "                tin_word_val_feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a18db99-3fad-499d-975a-a6c445fb8a62",
   "metadata": {},
   "source": [
    "Fitting a linear SVC and using GridSearchCV to find the optimal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3a9a5cb-b9e0-4d59-8203-f9f7f58b5c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=LinearSVC(), param_grid={'C': [0.1, 1, 10]},\n",
       "             scoring='f1_micro')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = LinearSVC()\n",
    "params = {'C':[.1,1,10]}\n",
    "clf = GridSearchCV(svc, params, scoring=\"f1_micro\",cv=3)\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d505633-a6d7-4a1f-b95f-2a0949d21293",
   "metadata": {},
   "source": [
    "Testing a Random Forest Classifier. This did not work as well as the LinearSVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a43fe53d-2633-48f0-8396-8f8d92fbae5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=RandomForestClassifier(),\n",
       "             param_grid={'n_estimators': [100]}, scoring='f1_micro')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "params = {'n_estimators':[100]}\n",
    "clf = GridSearchCV(rf, params, scoring=\"f1_micro\",cv=3)\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4415a8c3-21ec-4b1a-8249-edc1850472a6",
   "metadata": {},
   "source": [
    "Viewing results: Top f1 micro and f1 macro scores on training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb789848-0f76-48e4-a827-aaaa1a1773cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 100} 0.7072004028197382\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.41377608562896845"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(clf.best_params_,clf.best_score_)\n",
    "preds = clf.predict(X_train)\n",
    "np.mean(cross_val_score(svc,X_train,y_train,scoring=\"f1_macro\",cv=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f5b708-93e1-42e1-b384-bfaa2cb07a96",
   "metadata": {},
   "source": [
    "Testing model using only bag of words for prediction. Note that these cells are only meant to be run when the model is fit with the corresponding features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5a0b525c-6cb0-4cb5-ab83-dc2f3a41dfe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7224320241691843 0.4321556060686495\n",
      "F1 Micro: 0.7224\n",
      "F1 Macro: 0.4322\n",
      "Precision: 0.6731\n",
      "Recall: 0.4243\n"
     ]
    }
   ],
   "source": [
    "val_preds = clf.predict(X_val)\n",
    "micro = f1_score(y_val, val_preds, average='micro')\n",
    "macro = f1_score(y_val, val_preds, average='macro')\n",
    "print(micro,macro)\n",
    "precision = precision_score(y_val, val_preds, average = \"macro\") # Get scores using svm_test_predictions and y_test with the precision_score method\n",
    "recall = recall_score(y_val, val_preds, average = \"macro\")\n",
    "\n",
    "print(\"F1 Micro: {:.4f}\".format(micro))\n",
    "print(\"F1 Macro: {:.4f}\".format(macro))\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170eb0a3-81da-4e68-8cb9-a31692391fdb",
   "metadata": {},
   "source": [
    "Testing model using bag of words plus feature made from offensive word lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "80114277-ffc2-45b6-a7e1-b08c8f5adc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Micro: 0.7224\n",
      "F1 Macro: 0.4322\n",
      "Precision: 0.6731\n",
      "Recall: 0.4243\n"
     ]
    }
   ],
   "source": [
    "val_preds = clf.predict(X_val)\n",
    "micro = f1_score(y_val, val_preds, average='micro')\n",
    "macro = f1_score(y_val, val_preds, average='macro')\n",
    "precision = precision_score(y_val, val_preds, average = \"macro\") # Get scores using svm_test_predictions and y_test with the precision_score method\n",
    "recall = recall_score(y_val, val_preds, average = \"macro\")\n",
    "\n",
    "print(\"F1 Micro: {:.4f}\".format(micro))\n",
    "print(\"F1 Macro: {:.4f}\".format(macro))\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5147d360-ede3-4f3a-ac05-aa837e55da76",
   "metadata": {},
   "source": [
    "Testing model using bag of words plus exclamation feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bbb6ae61-0409-4b85-a409-c7bb6bcbffb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Micro: 0.7224\n",
      "F1 Macro: 0.4322\n",
      "Precision: 0.6731\n",
      "Recall: 0.4243\n"
     ]
    }
   ],
   "source": [
    "val_preds = clf.predict(X_val)\n",
    "micro = f1_score(y_val, val_preds, average='micro')\n",
    "macro = f1_score(y_val, val_preds, average='macro')\n",
    "precision = precision_score(y_val, val_preds, average = \"macro\") # Get scores using svm_test_predictions and y_test with the precision_score method\n",
    "recall = recall_score(y_val, val_preds, average = \"macro\")\n",
    "\n",
    "print(\"F1 Micro: {:.4f}\".format(micro))\n",
    "print(\"F1 Macro: {:.4f}\".format(macro))\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f887dfd0-ffd5-46df-a9b4-9dcf19d171a1",
   "metadata": {},
   "source": [
    "Testing model using bag of words plus Trump word count feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aab8b5bd-fc00-4c7b-8470-ae58277f0342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Micro: 0.7224\n",
      "F1 Macro: 0.4322\n",
      "Precision: 0.6731\n",
      "Recall: 0.4243\n"
     ]
    }
   ],
   "source": [
    "val_preds = clf.predict(X_val)\n",
    "micro = f1_score(y_val, val_preds, average='micro')\n",
    "macro = f1_score(y_val, val_preds, average='macro')\n",
    "precision = precision_score(y_val, val_preds, average = \"macro\") # Get scores using svm_test_predictions and y_test with the precision_score method\n",
    "recall = recall_score(y_val, val_preds, average = \"macro\")\n",
    "\n",
    "print(\"F1 Micro: {:.4f}\".format(micro))\n",
    "print(\"F1 Macro: {:.4f}\".format(macro))\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6c2042-0ff7-4ad2-934c-f8de1c7c811f",
   "metadata": {},
   "source": [
    "Testing bag of words with personal pronoun feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7202290b-7637-4e17-88fa-0667466ace05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Micro: 0.7224\n",
      "F1 Macro: 0.4322\n",
      "Precision: 0.6731\n",
      "Recall: 0.4243\n"
     ]
    }
   ],
   "source": [
    "val_preds = clf.predict(X_val)\n",
    "micro = f1_score(y_val, val_preds, average='micro')\n",
    "macro = f1_score(y_val, val_preds, average='macro')\n",
    "precision = precision_score(y_val, val_preds, average = \"macro\") # Get scores using svm_test_predictions and y_test with the precision_score method\n",
    "recall = recall_score(y_val, val_preds, average = \"macro\")\n",
    "\n",
    "print(\"F1 Micro: {:.4f}\".format(micro))\n",
    "print(\"F1 Macro: {:.4f}\".format(macro))\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f264cc4-693a-43f8-b56d-9187f7010c04",
   "metadata": {},
   "source": [
    "Bag of words with positive word feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "37ced5f7-9134-4763-919d-ee5595ef9b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Micro: 0.7224\n",
      "F1 Macro: 0.4322\n",
      "Precision: 0.6731\n",
      "Recall: 0.4243\n"
     ]
    }
   ],
   "source": [
    "val_preds = clf.predict(X_val)\n",
    "micro = f1_score(y_val, val_preds, average='micro')\n",
    "macro = f1_score(y_val, val_preds, average='macro')\n",
    "precision = precision_score(y_val, val_preds, average = \"macro\") # Get scores using svm_test_predictions and y_test with the precision_score method\n",
    "recall = recall_score(y_val, val_preds, average = \"macro\")\n",
    "\n",
    "print(\"F1 Micro: {:.4f}\".format(micro))\n",
    "print(\"F1 Macro: {:.4f}\".format(macro))\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70865cf7-d0b6-4e16-981f-7393fa4acaff",
   "metadata": {},
   "source": [
    "Bag of words with TIN feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "83ed7490-748f-40b8-808d-3a41c38abe4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Micro: 0.7224\n",
      "F1 Macro: 0.4322\n",
      "Precision: 0.6731\n",
      "Recall: 0.4243\n"
     ]
    }
   ],
   "source": [
    "val_preds = clf.predict(X_val)\n",
    "micro = f1_score(y_val, val_preds, average='micro')\n",
    "macro = f1_score(y_val, val_preds, average='macro')\n",
    "precision = precision_score(y_val, val_preds, average = \"macro\") # Get scores using svm_test_predictions and y_test with the precision_score method\n",
    "recall = recall_score(y_val, val_preds, average = \"macro\")\n",
    "\n",
    "print(\"F1 Micro: {:.4f}\".format(micro))\n",
    "print(\"F1 Macro: {:.4f}\".format(macro))\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af38765e-cdc2-4b8c-bad3-9839d216ef0e",
   "metadata": {},
   "source": [
    "All features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bbc4fc38-5db8-4905-baa7-a0293d28252c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Micro: 0.7224\n",
      "F1 Macro: 0.4322\n",
      "Precision: 0.6731\n",
      "Recall: 0.4243\n"
     ]
    }
   ],
   "source": [
    "val_preds = clf.predict(X_val)\n",
    "micro = f1_score(y_val, val_preds, average='micro')\n",
    "macro = f1_score(y_val, val_preds, average='macro')\n",
    "precision = precision_score(y_val, val_preds, average = \"macro\") # Get scores using svm_test_predictions and y_test with the precision_score method\n",
    "recall = recall_score(y_val, val_preds, average = \"macro\")\n",
    "\n",
    "print(\"F1 Micro: {:.4f}\".format(micro))\n",
    "print(\"F1 Macro: {:.4f}\".format(macro))\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ec333e-f62f-4e77-9661-ed11bfb27eac",
   "metadata": {},
   "source": [
    "Bag of words plus offensive word feature, targeted insult word feature and personal pronoun feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e23efe9d-a57a-41f4-b21c-9d0bebc51cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Micro: 0.7224\n",
      "F1 Macro: 0.4322\n",
      "Precision: 0.6731\n",
      "Recall: 0.4243\n"
     ]
    }
   ],
   "source": [
    "val_preds = clf.predict(X_val)\n",
    "micro = f1_score(y_val, val_preds, average='micro')\n",
    "macro = f1_score(y_val, val_preds, average='macro')\n",
    "precision = precision_score(y_val, val_preds, average = \"macro\") # Get scores using svm_test_predictions and y_test with the precision_score method\n",
    "recall = recall_score(y_val, val_preds, average = \"macro\")\n",
    "\n",
    "print(\"F1 Micro: {:.4f}\".format(micro))\n",
    "print(\"F1 Macro: {:.4f}\".format(macro))\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3332fb86-bd4d-4bed-8086-a2df88ab88ac",
   "metadata": {},
   "source": [
    "Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "df2b421d-e5f8-47dc-8fae-781392a3ff0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Micro: 0.7224\n",
      "F1 Macro: 0.4322\n",
      "Precision: 0.6731\n",
      "Recall: 0.4243\n"
     ]
    }
   ],
   "source": [
    "val_preds = clf.predict(X_val)\n",
    "micro = f1_score(y_val, val_preds, average='micro')\n",
    "macro = f1_score(y_val, val_preds, average='macro')\n",
    "precision = precision_score(y_val, val_preds, average = \"macro\") # Get scores using svm_test_predictions and y_test with the precision_score method\n",
    "recall = recall_score(y_val, val_preds, average = \"macro\")\n",
    "\n",
    "print(\"F1 Micro: {:.4f}\".format(micro))\n",
    "print(\"F1 Macro: {:.4f}\".format(macro))\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463ae0d7-2236-49ac-9380-528ac20561e2",
   "metadata": {},
   "source": [
    "Conducting error analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "06b0d485-b066-458e-9f40-209915d2858f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['@USER @USER @USER @USER @USER Oh and how do think the victim feels? Im sure she is fine with the death threats and insults sheâ€™s been getting spewed at her by you fine Christian conservatives',\n",
       "  'NOT'],\n",
       " ['@USER we bout to start stepping out lol I ainâ€™t doing another winter with no boo bro. Shit cold .... literally',\n",
       "  'TIN'],\n",
       " ['@USER  Good- that soundtrack you DMed me with was fire as fuck and I still listen to it oftenðŸ’ªðŸ½ Bad- My tracks are betterðŸ˜­ (joking Iâ€™m shit)',\n",
       "  'TIN'],\n",
       " ['@USER Oh shit.', 'TIN'],\n",
       " ['@USER sex games will make you feel like you\\'re actully having sex or, as ign would describe it it really makes you feel like you\\'re fucking some whore in an alleyway\"\"',\n",
       "  'NOT'],\n",
       " ['@USER A vulgar attempt to reduce sisterhood to genitalia.....', 'NOT'],\n",
       " ['Opportunist to the max. An evil lier also. URL', 'NOT'],\n",
       " [\"@USER Stop saying this shit you are not going to do it shrio you have to fight this depression you can't let it take you half of my life was me hiding my emotions everyone thought I was happy but deep down I wanted to end it all but I didn't let depression beat me I kept fighting.\",\n",
       "  'NOT'],\n",
       " ['@USER Go crash another plane.', 'NOT'],\n",
       " ['@USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER I just blocked 3 asses.',\n",
       "  'NOT'],\n",
       " [\"@USER Does one bring the other? #Sex #War See Penthouse magazine in the 1980's.  Sex followed by guns.\",\n",
       "  'NOT'],\n",
       " ['@USER Has your network/organization put out a report on actress Li BingBing???  Some reports say she is missing\", other reports say the Chinese Government has abducted her????   What the heck is going on????\"',\n",
       "  'NOT'],\n",
       " ['@USER @USER My favourite part of this is watching all the conservatives lose their minds as usual. Once again the Democrats are being mean to us boo-hoo. LOL.',\n",
       "  'NOT'],\n",
       " [\"All this talk about Trump's dick allegedly looking like Toad from #MarioKart  This is the only thing that has been accomplished. Only now the libs are picturing a 70 year old man naked.   #MAGA URL\",\n",
       "  'NOT'],\n",
       " ['Take Take Take #WalkAway from the #idiot party! #MAGA URL', 'NOT'],\n",
       " [\"@USER I've seen people with some the most beautiful (not to mention original) art i last my eyes on. but only have 70 followers because they get buried by half-assed bullshit. all because they're not pals with someone popular\",\n",
       "  'NOT'],\n",
       " ['@USER Monte... who fuckin knows at this point', 'NOT'],\n",
       " ['@USER @USER Liberals  wouldnâ€™t know the first thing about â€œdoing the right thingâ€ ðŸ¤£ðŸ¤£ðŸ¤£ðŸ–•ðŸ¼ðŸ–•ðŸ¼ðŸ–•ðŸ¼ðŸ–•ðŸ¼ðŸ–•ðŸ¼',\n",
       "  'NOT'],\n",
       " ['@USER @USER @USER @USER @USER Most people do not understand what an assault rifle is!  Makes their statements regarding gun control stupid!',\n",
       "  'NOT'],\n",
       " ['@USER Another elitist who is out of touch with the patriotic Americans!',\n",
       "  'NOT'],\n",
       " ['@USER #Causality? Is it:  1. Chicago lawmakers instituted strict #guncontrol BECAUSE of high pre-existing crime rate? 2. Crime rate is high BECAUSE strict gun control prevents law abiding citizens to be armed - so only scofflaws have guns? 3. No causality at all. Statistics are #COHORT',\n",
       "  'NOT'],\n",
       " [\"@USER yesterday this happen hereðŸ˜³ So much for gun control when you can't prevent a 2nd grader from bringing a gun to school huhðŸ˜ URL\",\n",
       "  'NOT'],\n",
       " ['100% TRUE! and she is stupid as well! URL', 'NOT'],\n",
       " ['@USER I ainâ€™t fucking w/u today lol', 'TIN'],\n",
       " ['@USER @USER Man Iâ€™m glad I can make yâ€™allâ€™s day. I just be saying stupid shit on the fly. I ainâ€™t trying Iâ€™m just dumb lmao URL',\n",
       "  'TIN'],\n",
       " ['@USER @USER #Westminster @USER #Tories @USER @USER @USER @USER Absolutely pathetic #appeasement ðŸ˜ ðŸ˜ ðŸ˜ ',\n",
       "  'TIN'],\n",
       " ['@USER @USER Horrendously sad that we all know who â€œheâ€ is.   It gives me no joy to know Iâ€™m smarter than the president.',\n",
       "  'NOT'],\n",
       " ['@USER Lmao I have it too. Shitty quality but I donâ€™t mind it either. The lowlands one sounds better ðŸ‘ðŸ»',\n",
       "  'NOT'],\n",
       " ['@USER Oh shit this will trigger some bigots', 'TIN'],\n",
       " ['@USER @USER She is better than all this shit. She is better than anything. â¤ï¸',\n",
       "  'TIN'],\n",
       " ['@USER They could literally give two fucks af ðŸ˜‚ like ummm okay thanks ðŸ˜‚',\n",
       "  'NOT'],\n",
       " ['But but #Beeto has the Drunk Drivers Who Leave The Scene Of An Accident vote all sewed up! #MAGA @USER #TxSen URL',\n",
       "  'NOT'],\n",
       " ['@USER Total Bullshit', 'NOT'],\n",
       " ['I love it when a plan comes together â˜ºï¸ #maga #nexit URL', 'NOT'],\n",
       " ['@USER @USER Gail has dropped far if she is going after actors like Rob to deal with the frustration of being a has-been...',\n",
       "  'NOT'],\n",
       " ['@USER He is so takendted what the heck', 'NOT'],\n",
       " ['@USER Disgusting', 'TIN'],\n",
       " ['@USER @USER bt1100 ass malfunction', 'NOT'],\n",
       " [\"@USER @USER At least you got to experience the 90s though. These 90s and 2000s born missed out on a great decade. It's been an increasingly worse shit show since lol\",\n",
       "  'TIN'],\n",
       " ['@USER I know what that storm is gonna do...but those photos are just the shit bro.',\n",
       "  'TIN'],\n",
       " [\"@USER @USER It takes a 2sec Google search to see who this accuser is - I'll help the mindless uneducated liberals . URL\",\n",
       "  'NOT'],\n",
       " ['@USER Fake News', 'NOT'],\n",
       " [\"@USER Don't dignify her stupidity by posting her comments!!!\", 'NOT'],\n",
       " ['Just imagine the tyranny weâ€™d be living under had these leftwing tryants waging war on us and our duly elected president and the American Republic itself gained tyrannical control of the US Supreme Court the neoconâ€™s #NeverTrump crew was trying to deliver to them?  #MAGA ðŸ‡ºðŸ‡¸',\n",
       "  'NOT'],\n",
       " ['@USER He is about as useless as a tit on a hog just saying ðŸ˜Ž', 'NOT'],\n",
       " ['@USER @USER I believe gun control should consist of guarding your firearms from thivery and kids.',\n",
       "  'NOT'],\n",
       " ['@USER What What In The Butt', 'NOT'],\n",
       " ['@USER Someone should\\'veTaken\" this piece of shit to a volcano. ðŸ˜‚\"', 'TIN'],\n",
       " ['@USER Yeah we need some more made up bullshit protestors and antifa lol time for an epic beatdownðŸ˜‰',\n",
       "  'NOT'],\n",
       " ['@USER shit ainâ€™t gone work bruh URL', 'NOT']]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = zip(list(val_preds),list(y_val))\n",
    "\n",
    "untCorrect = []\n",
    "untIncorrect = []\n",
    "\n",
    "for i in range(len(val_preds)):\n",
    "    if y_val[i] == \"UNT\":\n",
    "        if val_preds[i] == \"UNT\":\n",
    "            untCorrect.append([X_val_txt[i]])\n",
    "        else:\n",
    "            untIncorrect.append([X_val_txt[i],val_preds[i]])\n",
    "            \n",
    "untIncorrect[:50]    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a737b364-c55e-4b60-971f-c86d89d6119c",
   "metadata": {},
   "source": [
    "Reading in the test data to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "254a9156-be8c-48b0-ad16-4505ab2b955f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Dict {'NOT': 2418, 'TIN': 223, 'UNT': 7}\n",
      "val dict {'NOT': 2419, 'TIN': 214, 'UNT': 15}\n",
      "grnd tru {'NOT': 1767, 'TIN': 776, 'UNT': 105}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.00340522133938706"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testPreds = clf.predict(X_test)\n",
    "\n",
    "testDict = {}\n",
    "for val in testPreds:\n",
    "    testDict[val] = testDict.get(val,0) + 1\n",
    "    \n",
    "print(\"test Dict\", testDict)\n",
    "\n",
    "valDict = {}\n",
    "for val in val_preds:\n",
    "    valDict[val] = valDict.get(val,0) + 1\n",
    "\n",
    "print(\"val dict\", valDict)\n",
    "\n",
    "valTruthDict = {}\n",
    "for val in y_val:\n",
    "    valTruthDict[val] = valTruthDict.get(val,0) + 1\n",
    "\n",
    "print(\"grnd tru\",valTruthDict)\n",
    "\n",
    "9/(2064+570+9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834a8e6a-1dde-40b4-9b04-e2d3c806a45b",
   "metadata": {},
   "source": [
    "Writing our test predictions to a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3841885c-795a-4f46-85fa-30e1f732654a",
   "metadata": {},
   "outputs": [],
   "source": [
    "oFile = open(\"predictions.tsv\",'w', encoding=\"utf8\")\n",
    "\n",
    "tsvObj = csv.writer(oFile,delimiter='\\t')\n",
    "listForWriting = []\n",
    "for i in range(len(X_text_test)):\n",
    "    listForWriting.append([X_text_test[i],testPreds[i]])\n",
    "\n",
    "tsvObj.writerows(listForWriting)\n",
    "\n",
    "oFile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
